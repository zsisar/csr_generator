{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "csr_ids_list = ['A3921104','B765009','C2005671']\n",
        "subsections_list = ['adverse event of special interest','evaluation of each lab parameter','lab values over time','safety lab']\n",
        "\n",
        "protocol_filter_map = defaultdict(dict)\n",
        "\n",
        "for csr_id in csr_ids_list:\n",
        "    section2list_map = {'adverse event of special interest': ['heart failure'],'evaluation of each lab parameter':['platelet'],'lab values over time':['whaterver'],'safety lab':['bla']}\n",
        "    protocol_filter_map[csr_id]= { subsection: values_list for subsection,values_list in section2list_map.items() }"
      ],
      "metadata": {
        "id": "gq6j_6NMuTYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict(protocol_filter_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thkmxMGNUjaY",
        "outputId": "ec1d08aa-b1ad-4963-c48f-6c13db84cf8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A3921104': {'adverse event of special interest': ['heart failure'],\n",
              "  'evaluation of each lab parameter': ['platelet'],\n",
              "  'lab values over time': ['whaterver'],\n",
              "  'safety lab': ['bla']},\n",
              " 'B765009': {'adverse event of special interest': ['heart failure'],\n",
              "  'evaluation of each lab parameter': ['platelet'],\n",
              "  'lab values over time': ['whaterver'],\n",
              "  'safety lab': ['bla']},\n",
              " 'C2005671': {'adverse event of special interest': ['heart failure'],\n",
              "  'evaluation of each lab parameter': ['platelet'],\n",
              "  'lab values over time': ['whaterver'],\n",
              "  'safety lab': ['bla']}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "protocol_df (MANUALLY FILL)=>\n",
        "| csr_id   | section_name                       | keywords_filter\n",
        "| A3921104 | adverse event of special interest  | [heart failure, gastro]\n",
        "| A3921104 | brief summary of special interest  |\n",
        "| A3921104 | vitals                             |\n",
        "| A3921104 | lab values                         |\n",
        "\n",
        "\n",
        "mapping_df =>\n",
        "| csr_id   | section_name                      | html_files_paths   | pdf_files_paths\n",
        "| A3921104 | adverse event of special interest |[list of tables ]   | [list of tables ]\n",
        "| A3921104 | brief summary of special interest |[list of tables ]   | [list of tables ]\n",
        "| A3921104 | vitals                            |[list of tables ]   | [list of tables ]\n",
        "| A3921104 | lab values                        |[list of tables ]   | [list of tables ]\n",
        "\n",
        "merged_df =>\n",
        "| csr_id   | section_name                      | html_files_paths   | pdf_files_paths      | keywords_filter\n",
        "| A3921104 | adverse event of special interest |[list of tables ]   | [list of tables ]    | [heart failure, gastro]\n",
        "| A3921104 | brief summary of special interest |[list of tables ]   | [list of tables ]\n",
        "| A3921104 | vitals                            |[list of tables ]   | [list of tables ]\n",
        "| A3921104 | lab values                        |[list of tables ]   | [list of tables ]\n",
        "\n",
        "\n",
        "merged_df PLUS prompts =>\n",
        "| csr_id   | section_name                      | html_files_paths   | pdf_files_paths      | keywords_filter          | system_mssg   | instruction_mssg\n",
        "| A3921104 | adverse event of special interest |[list of tables ]   | [list of tables ]    | [heart failure, gastro]  |               |\n",
        "| A3921104 | brief summary of special interest |[list of tables ]   | [list of tables ]\n",
        "| A3921104 | vitals                            |[list of tables ]   | [list of tables ]\n",
        "| A3921104 | lab values                        |[list of tables ]   | [list of tables ]\n",
        "\n",
        "\n",
        "prompt_df =>\n",
        "section_name | system_mssg1 | instruction_mssg1| few_shot1 |system_mssg2 | instruction_mssg2| few_shot2\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "MMIpNtgNUqKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### let x be the pickle file ###\n",
        "protocol_df = pd.read_csv('PATH_HERE')"
      ],
      "metadata": {
        "id": "Krq06BiQolje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### part-1 (PROTOCOL BASED POST-PROCESSING): Given all tables keeping only the releavant rows of the table for AESI /LABS using PROTOCOL and saving row_filtered_dfs in same pickle file x under key table_content_processed ###\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "for csr_id in csr_ids_list:\n",
        "\n",
        "  csr_df = mapping_df[ mapping_df['csr_id'] == csr_id]\n",
        "  csr_df = pd.merge(csr_df, protocol_df, how ='left', on = ['csr_id','section_name'])\n",
        "\n",
        "  for i in range(len(csr_df)):\n",
        "\n",
        "    section_name  = csr_df.iloc[i].section_name\n",
        "    if section_name not in ['adverse event of special interest','evaluation of each lab parameter','lab values over time','safety lab']:\n",
        "      continue\n",
        "\n",
        "    keywords_filter_list = csr_df.iloc[i].keywords_filter\n",
        "    pdf_file_paths = csr_df.iloc[i].pdf_files_paths\n",
        "    html_file_paths = csr_df.iloc[i].html_files_paths\n",
        "\n",
        "    ############# HTML CASE #############\n",
        "    for file_path in html_file_paths:\n",
        "\n",
        "      table_df = x[file_path]['table_content'] ### ASSUMING THIS HAS TO BE A DATAFRAME RIGHT NOW ###\n",
        "\n",
        "      ####### needs to change => col_names are there in header only #########\n",
        "      filtered_table_rows_df_list = []\n",
        "\n",
        "      for i in range(len(table_df)):\n",
        "\n",
        "\n",
        "        ######## PLS QC IF THIS IS WORKING OR NOT ###########\n",
        "        if table_df.iloc[i, 0:2].str.lower().contains(r'\\b(?:{})\\b'.format('|'.join(keywords_filter_list).lower()))\n",
        "          filtered_table_rows_df_list.append(table_df.iloc[i])\n",
        "\n",
        "      #### set col headers as table_df headers ####\n",
        "      if filtered_table_rows_df_list:\n",
        "        table_df_filtered = pd.concat(filtered_table_rows_df_list)\n",
        "        table_df_filtered.columns = table_df.columns\n",
        "\n",
        "      else:\n",
        "        table_df_filtered = table_df\n",
        "\n",
        "      x[file_path]['table_content_processed'] = table_df_filtered\n",
        "\n",
        "    ############# PDF CASE #############\n",
        "    for file_path in pdf_file_paths:\n",
        "\n",
        "      table_df_list = x[file_path]['table_content'] ### ASSUMING THIS HAS TO BE A LIST OF DATAFRAMES RIGHT NOW =>>> NEED TO CHECK THIS ###\n",
        "      output_list = []\n",
        "\n",
        "      for table_df in table_df_list:\n",
        "\n",
        "        filtered_table_rows_df_list = []\n",
        "\n",
        "        ###### PDF tables have 1st row as headers ##################\n",
        "        filtered_table_rows_df_list.append(table_df.iloc[0])\n",
        "\n",
        "        for i in range(1, len(table_df)):\n",
        "\n",
        "          if table_df.iloc[i, 0:2].str.lower().contains(r'\\b(?:{})\\b'.format('|'.join(keywords_filter_list).lower()))\n",
        "            filtered_table_rows_df_list.append(table_df.iloc[i])\n",
        "\n",
        "        if filtered_table_rows_df_list>1:\n",
        "          table_df_filtered = pd.concat(filtered_table_rows_df_list)\n",
        "\n",
        "        else:\n",
        "          table_df_filtered = table_df\n",
        "\n",
        "        output_list.append(table_df_filtered)\n",
        "\n",
        "\n",
        "      x[file_path]['table_content_processed'] = output_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OH1IZ_GAlkfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### part 2 : COMBINED PIPELINE ###\n",
        "prompt_df = pd.read_csv('../PATH_TO_PROMPT_DF../')\n",
        "\n",
        "for csr_id in csr_ids_list:\n",
        "\n",
        "\n",
        "  csr_df = mapping_df[mapping_df['csr_id'] == csr_id]\n",
        "  csr_df = pd.merge(csr_df, protocol_df, how ='left', on = ['csr_id','section_name'])\n",
        "  csr_df = pd.merge(csr_df, prompt_df, on='section_name', how ='left')\n",
        "\n",
        "  output_csr  = ''\n",
        "\n",
        "  for i in range(len(csr_df)):\n",
        "\n",
        "    ####### one section of a CSR at a time ###########\n",
        "    section_name = csr_df.iloc[i].section_name\n",
        "    pdf_file_paths = csr_df.iloc[i].pdf_files_paths\n",
        "    html_file_paths = csr_df.iloc[i].html_files_paths\n",
        "    keywords_list = csr_df.iloc[i].keywords_filter\n",
        "\n",
        "    system_mssg1 = csr_df.iloc[i].system_mssg1\n",
        "    few_shot1 = csr_df.iloc[i].few_shot1\n",
        "    instruction_mssg1 = csr_df.iloc[i].instruction_mssg1\n",
        "    ############## QC THIS STEP  ##############\n",
        "    instruction_mssg1 = instruction_mssg1.format(keywords = keywords_list)\n",
        "\n",
        "    system_mssg2 = csr_df.iloc[i].system_mssg2\n",
        "    few_shot2 = csr_df.iloc[i].few_shot2\n",
        "    instruction_mssg2 = csr_df.iloc[i].instruction_mssg2\n",
        "    ############# QC THIS STEP  ###############\n",
        "    instruction_mssg2 = instruction_mssg2.format(keywords = keywords_list)\n",
        "\n",
        "    ### CASE 1 ###\n",
        "    if len(pdf_file_paths)>0 and len(html_file_paths)>0:\n",
        "      ######## CAUTION: IN CASE OF PDFS EACH PDF FILE PATH WILL HAVE A LIST OF DFS NOT A SINGLE DF ############\n",
        "      output_csr += generate_pdfs_output(pdf_file_paths, system_mssg1, instruction_mssg1, few_shot1, system_mssg2, instruction_mssg2, few_shot2)\n",
        "      output_csr += generate_htmls_output(html_file_paths,system_mssg1, instruction_mssg1, few_shot1, system_mssg2, instruction_mssg2, few_shot2)\n",
        "\n",
        "    ### CASE 2 ###\n",
        "    elif len(pdf_file_paths)>0 and len(html_file_paths)==0:\n",
        "      output_csr += generate_pdfs_output(pdf_file_paths, system_mssg1, instruction_mssg1, few_shot1, system_mssg2, instruction_mssg2, few_shot2)\n",
        "\n",
        "      ### only do this if table name does not contain descriptive stats/ listing/ categorization ###\n",
        "      pdf_file_paths = [ path for path in pdf_file_paths if 'descriptive' not in path and 'listing' not in path and'categorization' not in path]\n",
        "      output_csr += generate_htmls_output_pdf(pdf_file_paths, system_mssg1, instruction_mssg1, few_shot1, system_mssg2, instruction_mssg2, few_shot2)\n",
        "\n",
        "\n",
        "    ### CASE 3 ###\n",
        "    elif len(pdf_file_paths)==0 and len(html_file_paths)>0:\n",
        "      output_csr += generate_htmls_output(html_file_paths, system_mssg1, instruction_mssg1, few_shot1, system_mssg2, instruction_mssg2, few_shot2)\n",
        "\n",
        "    else:\n",
        "      output_csr += f\"{section_name} \\nThis section is not applicable.\"\n",
        "\n",
        "    ### end of one section of this csr ###\n",
        "    output_csr  += '\\n\\n\\n\\n'\n",
        "\n",
        "  ###save this output csr ###\n",
        "  file = open(f'Study_Report_{csr_id}.txt', 'w')\n",
        "  file.write(output_csr)\n",
        "  file.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "GH4bMHgzNval"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "\n",
        "def prompt_pdf(table_list):\n",
        "\n",
        "  api_key = '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'\n",
        "  openai.api_key = api_key\n",
        "\n",
        "  prompt = f'''You are provided with a list of table names in the variable 'table_list'.Follow the instructions to provided the summary text of all tables in the list. Do not make up answers. Do not elaborate. Do not use bullet point.\n",
        "             Tasks:\n",
        "             1. If multiple tables share some identical words, consider them belong to the same category. Otherwise, consider the table as a category by itself. Show me the tables each category contains.\n",
        "             2. For categories having multiple tables, summarize in the same format as the example below for each category in the following format: Start with the identical parts of the table name, followed by the different part in parentheses, followed the table prefix (such as Table 1.2.3.4). Repeat the written summary for each table.\n",
        "             3. For category with only one table, report the exact table name followed by table prefix.\n",
        "             4. Only need to report the final summary. No need to mention details of how tables are categorized. Report only the Summary variable.\n",
        "\n",
        "             table_list:{table_list}\n",
        "\n",
        "             Below is an example of the expected output with input table list.\n",
        "             Input: ['Table 14.3.4.1.16.1 Absolute values in Hemoglobin (Up to Week 16) ', 'Table 14.3.4.1.16.2 Absolute values in Hemoglobin (all visits)']\n",
        "             Output: Absolute values in hemoglobin are provided by visit in Table 14.3.4.1.16.1 (up to Week 16) and Table 14.3.4.1.16.2 (all visits)\n",
        "             '''\n",
        "\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages = [{'role': 'user',\n",
        "                  'content': prompt}],\n",
        "      max_tokens=526,\n",
        "      temperature=0.0,\n",
        "      n = 1,\n",
        "      stop=None\n",
        "  )\n",
        "\n",
        "  generated_text = response['choices'][0]['message']['content']\n",
        "  return generated_text\n",
        "\n",
        "  '''\n",
        "  intext tables prep from PDFs (assumption)\n",
        "  only PDFs...\n",
        "  HTM TABLES CREATED FROM PDFS summarizing across multiple PDFs ( has to come from some PDF may be in multiple PDFs)\n",
        "  - Listings by subject ->each pg for 1 subject (demoraphic info)\n",
        "\n",
        "  IF NO HTML:\n",
        "  - what assumption are we making here????\n",
        "  - what kinda tables do we want to consider here to actually create the summary????\n",
        "  - test out assumption with CSRs (NO HTMS) => Is prompt suited OR NOT\n",
        "  - use one set of prompts for HTMs / one for PDFs\n",
        "  -\n",
        "  '''\n"
      ],
      "metadata": {
        "id": "W0FHFNT6aB9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pdfs_output(pdf_file_paths, system_mssg1, instruction_mssg1, few_shot1, system_mssg2, instruction_mssg2, few_shot2):\n",
        "  ### only table names required hehe ###\n",
        "  pdf_table_names = [clean_table_name(path) for path in pdf_file_paths]\n",
        "  return prompt_pdf(pdf_table_names)"
      ],
      "metadata": {
        "id": "UjcEsNZuY9rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' RIGHT NOW in post-processing if no relevant row is found.. entire tables are getting returned. IS THAT CORRECT?????????'''\n",
        "\n",
        "def generate_htmls_output(html_file_paths, system_mssg1, instruction_mssg1, few_shot1, system_mssg2, instruction_mssg2, few_shot2):\n",
        "\n",
        "  '''INPUT_ARGS: html_file_paths => all HTML paths corresponding to one section of one CSR (say html paths of all brief summary of ae tables)\n",
        "     OUTPUT: Output is CSR text for one section using the system_mssg & instruction_mssg'''\n",
        "\n",
        "  output = ''\n",
        "\n",
        "  for html_file_path in html_file_paths:\n",
        "    if len(x[html_file_path]['table_content_processed']) == 0:\n",
        "      output += f\"No significant data is found in this table {x[html_file_path]['table_name']}\"\n",
        "\n",
        "\n",
        "  ###### for one section (say brief summary of ae) all htm tables are passed into context as list format #######\n",
        "  context_list = [ \"######### table title: \"+ x[path]['table_name'] + '\\ntable: '+ x[path]['table_content_processed'].to_mark_down() + '\\n' for path in html_file_paths ]\n",
        "  try:\n",
        "    complete_context = '\\n\\n'.join(context_list)\n",
        "    output += generate_vox_output(system_mssg1, instruction_mssg1, few_shot1, complete_context)\n",
        "    ######## HOW TO HANDLE THE MODEL CHANGE PART IN generate_vox_output ???????????? AS BEFORE IN ERR CASE WE WERE CHANGING MODEL ?????????????##################\n",
        "\n",
        "  except Exception as e:\n",
        "    ############# now pass one by one ########################\n",
        "    ###### concatenating/appending the outputs of each file each table ...into output result #######\n",
        "    for context in context_list:\n",
        "      output += generate_vox_output(system_mssg2, instruction_mssg2, few_shot2, context)\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "eFsxZEXobi7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_vox_output(system_mssg1, instruction_mssg1, few_shot1, complete_context):\n",
        "  try:\n",
        "\n",
        "  except Exception as e:\n",
        "    print('VOX errror: ',e)\n",
        "    ###########CHANGE MODEL HERE CODE MADE BEFORE ###################\n"
      ],
      "metadata": {
        "id": "6iQPwRGu3VLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' RIGHT NOW in post-processing if no relevant row is found.. entire tables are getting returned. IS THAT CORRECT?????????'''\n",
        "\n",
        "def generate_htmls_output_pdf(pdf_file_paths, system_mssg, instruction_mssg, few_shot):\n",
        "\n",
        "  '''INPUT_ARGS: html_file_paths => all HTML paths corresponding to one section of one CSR (say html paths of all brief summary of ae tables)\n",
        "     OUTPUT: Output is CSR text for one section using the system_mssg & instruction_mssg'''\n",
        "\n",
        "  output = ''\n",
        "\n",
        "  for html_file_path in pdf_file_paths:\n",
        "    if len(x[pdf_file_paths]['table_content_processed']) == 0:\n",
        "      output += f\"No significant data is found in this table {x[pdf_file_paths]['table_name']}\"\n",
        "\n",
        "  ############# ADD CODE FROM VOX HERE #####################\n",
        "\n",
        "  context_list = []\n",
        "  for path in pdf_file_paths:\n",
        "    table_df_list = x[path]['table_content_processed']\n",
        "\n",
        "    table_name = x[path]['table_name']\n",
        "    for table_df in table_df_list:\n",
        "      context_list.append(\"######### table title: \"+ table_name + '\\ntable: '+ table_df.to_mark_down() + '\\n')\n",
        "\n",
        "  try:\n",
        "    complete_context = '\\n\\n'.join(context_list)\n",
        "    output += generate_vox_output(system_mssg, instruction_mssg, few_shot, complete_context)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    ########## now pass one by one ################\n",
        "    ###### concatenating/appending the outputs of each file each table ...into output result #######\n",
        "    for context in context_list:\n",
        "      output += generate_vox_output(system_mssg2, instruction_mssg2, few_shot2, context)\n",
        "\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "lOvUjIcUq1FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8xdrKZUZL9X",
        "outputId": "049130f3-2b9b-4a55-8668-b51050ec7403"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def generate_table_name_summary(title):\n",
        "  api_key = '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'\n",
        "  openai.api_key = api_key\n",
        "\n",
        "  prompt = f'''Using the table title summarize what is provided in the table. Be sure to reference the table number.\n",
        "title: Table 14.3.4.1.16.1 Absolute values in Hemoglobin (Up to Week 16)\n",
        "output: Absolute values in hemoglobin are provided by visit in Table 14.3.4.1.16.1 (up to Week 16)\n",
        "title: {title}\n",
        "output:'''\n",
        "\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages = [{'role': 'user',\n",
        "                  'content': prompt}],\n",
        "      max_tokens=526,\n",
        "      temperature=0.0,\n",
        "      n = 1,\n",
        "      stop=None\n",
        "  )\n",
        "\n",
        "  generated_text = response['choices'][0]['message']['content']\n",
        "  return generated_text\n"
      ],
      "metadata": {
        "id": "DqdhDEYnXUt_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_table_name_summary('Table 8. Permanent Discontinuation Due to Adverse Event')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8DAtV2btZGDx",
        "outputId": "b959262c-31dd-425d-e552-d2b2f452b1dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Table 8 provides information on permanent discontinuation due to adverse events.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_table_name_summary('Table 32. Summary of SUbjects who Discontinued from Study Due to Adverse Events in Open-Label Run-in Phase OLFAS')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "B3CaEOW5eUmF",
        "outputId": "178c72cf-0e20-4f58-9f4b-9c62e66d605b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Table 32 provides a summary of subjects who discontinued from the study due to adverse events in the Open-Label Run-in Phase OLFAS.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### generate_htmls_output -> take from VOX\n",
        "### make sure to take table df from table_content_processed instead of table_content if key is present IN generate_htmls_output()"
      ],
      "metadata": {
        "id": "fzVqoTDTXKaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt1 = \"My name\""
      ],
      "metadata": {
        "id": "kYSNK3GmhTqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt1.format(fname = \"John\", age = 36)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "17hSx5xwhUQz",
        "outputId": "3e6215f8-dfd4-4917-8ebe-7058f6c99e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My name'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xlphK6PiiQmR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}